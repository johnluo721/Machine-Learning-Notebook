{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Minning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Expression  \n",
    "Regular Expression used to find the match pattern from text. It requires import package re.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Regular Expression cheatsheet.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datetime Operations\n",
    "\n",
    "**Convert string to Datetime format**  \n",
    "\n",
    "**Extract Year, Month, Day**  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data=pd.read_csv(\"/Users/johnluo/Desktop/Python/Dataset/Australia.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words\n",
    "##\n",
    "Bag of words does not consider the context of text, it only tokenize the words and counts its' frequency as feature.  \n",
    "\n",
    "sklearn.feature_extraction.text.CountVectorizer  \n",
    "sklearn.feature_extraction.text.TfidfVectorizer\n",
    "\n",
    "\n",
    "**fit_transform()** tokenize words and summarize frequency- return a   \n",
    "**get_feature_names()** get features or token  \n",
    "**toarray()** Convert to transform result to ndarray format.   \n",
    "\n",
    "**TF-IDF Calculation**  \n",
    "TF-IDF=TF*IDF, Index which calculate feature important across all documents.  \n",
    "\n",
    "TF- Term Frequency:  \n",
    "if a word repeated/high frequency than others, it will have high descrimination power. However if the term have high frequency across all courpos, it means the words is too common, and can not used to distinguish difference(like stop words, the, an , of,etc.). \n",
    "TF= word frequency across all courpos/total number of words across all document.   \n",
    "\n",
    "IDF- Inverse Document Frequency:   \n",
    "idf = log(n / docs(w, D))   \n",
    "\n",
    "The goal of TF-IDF is to remove words which are too common across all documents, and keep words which have more descrimination power over others. \n",
    "\n",
    "\n",
    "tfidf vectorizer have options:  \n",
    "mind_df: set minimum docment frequency, to remove words only apear in a few docs. set this threshold to limit vector dimension  \n",
    "max_df: oppose of mind_df  \n",
    "ngram: This option default as one, which treat every single word as a token. Set nagram(min, max) to allow look bigram or trigram. \"not bad\" as 2 gram will have totally opposite meanning of \"bad\" by itself as 1 gram. The downside of it is it will increase the vector demension.   \n",
    "\n",
    "Checklist to improve prediction of text mining model  \n",
    "1. Set ngram option  \n",
    "2. Set min_df to remove words which does not occur too often. \n",
    "2. Remove unwanted strings, such as special characters, digit, etc.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neutral Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Most popular neutral language processing package is nltk\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "wordnet complie group of words based on their meaning, synonyms or antonyms. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, you're going to need to import wordnet:\n",
    "from nltk.corpus import wordnet\n",
    " \n",
    "# Then, we're going to use the term \"program\" to find synsets like so:\n",
    "syns = wordnet.synsets(\"program\")\n",
    " \n",
    "# An example of a synset:\n",
    "print(syns[0].name())\n",
    " \n",
    "# Just the word:\n",
    "print(syns[0].lemmas()[0].name())\n",
    " \n",
    "# Definition of that first synset:\n",
    "print(syns[0].definition())\n",
    " \n",
    "# Examples of the word in use in sentences:\n",
    "print(syns[0].examples())\n",
    "\n",
    "synonyms = []\n",
    "antonyms = []\n",
    " \n",
    "for syn in wordnet.synsets(\"good\"):\n",
    "    for l in syn.lemmas():\n",
    "        synonyms.append(l.name())\n",
    "        if l.antonyms():\n",
    "            antonyms.append(l.antonyms()[0].name())\n",
    " \n",
    "print(set(synonyms))\n",
    "print(set(antonyms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modeling - gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gensim is a python library which specialised on Topic- modelling, including LDA model. It can use together with NLTK package. \n",
    "\n",
    "General approach\n",
    "\n",
    "1. use nltk CountVectorizer.fit_transform to tokenize words in text.   \n",
    "2. use gensim.matutils.Sparse2Corpus() change nltk token to gensim corpus.   \n",
    "3. Mapping from word IDs to words (To be used in LdaModel's id2word parameter) -id_map = dict((v, k) for k, v in vect.vocabulary_.items())\n",
    "4. train LDA model:  ldamodel=gensim.models.ldamodel.LdaModel(corpus,num_topics=10,id2word=id_map ,passes=25 , random_state=34)\n",
    "5. display topics and top words in these topics: ldamodel.show_topics(num_topics=10, num_words=10, log=False, formatted=True)\n",
    "\n",
    "Classify new corpus\n",
    "repeat step 1&2, and then use ldamodel.get_document_topics(new_corpus)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference\n",
    "\n",
    "NLTK blog\n",
    "http://www.coderjie.com/blog/8658d836c36111e6841d00163e0c0e36\n",
    "\n",
    "\n",
    "gensim official website\n",
    "https://radimrehurek.com/gensim/tutorial.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
